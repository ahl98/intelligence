{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "Deep Visual-Semantic Alignments for Generating Image Descriptions",
    "type": "sigai_heading"
   },
   "source": [
    "<img src=\"https://ucfai.org//intelligence/sp19/image-description-generation/banner.jpg\">\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <a class=\"btn btn-success btn-block\" href=\"https://ucfai.org/signup\">\n",
    "        First Attendance? Sign Up!\n",
    "    </a>\n",
    "</div>\n",
    "\n",
    "<div class=\"col-12\">\n",
    "    <h1> Deep Visual-Semantic Alignments for Generating Image Descriptions </h1>\n",
    "    <hr>\n",
    "</div>\n",
    "\n",
    "<div style=\"line-height: 2em;\">\n",
    "    <p>by: \n",
    "        <strong> Evan Waldmann</strong>\n",
    "        (<a href=\"https://github.com/Waldmannly\">@Waldmannly</a>)\n",
    "    \n",
    "        <strong> John Muchovej</strong>\n",
    "        (<a href=\"https://github.com/ionlights\">@ionlights</a>)\n",
    "     on 2019-03-25</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "ucfai": {
   "authors": [
    {
     "author": "Evan Waldmann",
     "github": "Waldmannly",
     "web": null
    },
    {
     "author": "John Muchovej",
     "github": "ionlights",
     "web": null
    }
   ],
   "date": "2019-03-25",
   "description": "Abstract: We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions tolearn about the inter-modal correspondences between  language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the  two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions.  We demonstrate that  our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCOdatasets. We then show that the generated descriptions sig outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",
   "tags": [],
   "title": "Deep Visual-Semantic Alignments for Generating Image Descriptions"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
